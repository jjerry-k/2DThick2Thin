{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tensorflow.keras import models, optimizers, losses\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from loss import *\n",
    "from utils import *\n",
    "from network import *\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fn(pred, label):\n",
    "#     if mode != 'test':\n",
    "        \n",
    "    plt.figure(figsize=(16, 16))\n",
    "    for i in range(12):\n",
    "        plt.subplot(4, 6, i+1)\n",
    "        plt.xlabel(\"%f\"%(pred[0,...,i].max()))\n",
    "        plt.imshow(pred[0,...,i], cmap='gray', vmin = 0, vmax = label.max())\n",
    "        plt.subplot(4, 6, i+1+12)\n",
    "        plt.title(\"%f\"%(label[0,...,i].max()))\n",
    "        plt.imshow(label[0,...,i], cmap='gray', vmin = 0, vmax = label.max())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = generator(model='dense', n_slice=12, case=2)\n",
    "\n",
    "print(\"\\n===================================\\n\")\n",
    "\n",
    "D = discriminator(model='dense', n_slice=12, case=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gen2Dis(gen, dis, case=1):\n",
    "    if case==2:\n",
    "        dis_out, texture = dis(gen.output)\n",
    "        net = models.Model(inputs=gen.input, outputs=[gen.output, dis_out])\n",
    "    elif case==3:\n",
    "        dis_out = dis(gen.output)\n",
    "        net = models.Model(inputs=gen.input, outputs=[gen.output, dis_out])\n",
    "    else:\n",
    "        dis_out = dis(gen.output[0])\n",
    "        net = models.Model(inputs=gen.input, outputs=[gen.output[0], dis_out])\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.trainable=False\n",
    "A = Gen2Dis(G, D, case=3)\n",
    "A.compile(optimizer=optimizers.Adam(lr=0.0001, epsilon=1e-8), \n",
    "          loss=[mutual_information, losses.binary_crossentropy], loss_weights=[100, 1])\n",
    "\n",
    "D.trainable=True\n",
    "D.compile(optimizer=optimizers.Adam(lr=0.0001, epsilon=1e-8), loss=losses.binary_crossentropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_low, train_high, val_low, val_high, test_low = data_loader_v3('./interpolation')\n",
    "print(\"Train X's shape : \", train_low.shape)\n",
    "print(\"Train Y's shape : \", train_high.shape)\n",
    "print(\"Validation X's shape : \", val_low.shape)\n",
    "print(\"Validation Y's shape : \", val_high.shape)\n",
    "print(\"Test X's shape : \", test_low.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_idx = np.random.choice(len(train_low), len(train_low), replace=False)\n",
    "train_low = train_low[shuffle_idx]\n",
    "train_high = train_high[shuffle_idx]\n",
    "batch = 2\n",
    "steps = len(train_low)//batch +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1500\n",
    "train_loss = {\"Generator_Total\" : [], \"Generator_Style\" : [], \"Generator_AD\" : [], \"Discriminator_AD\" : []}\n",
    "val_loss = {\"Generator_Total\" : [], \"Generator_Style\" : [], \"Generator_AD\" : []}\n",
    "\n",
    "total_progbar = Progbar(epochs)\n",
    "A.save(\"./checkpoint/3to12/dense_MI/model.h5\")\n",
    "for epoch in range(epochs):\n",
    "    #print(\"Epochs : %03d/%03d\"%(epoch+1, epochs))\n",
    "    epoch_progbar = Progbar(steps)\n",
    "    epoch_t_g_total = 0\n",
    "    epoch_t_g_style = 0\n",
    "    epoch_t_g_dis = 0\n",
    "    epoch_t_d_dis = 0\n",
    "    \n",
    "    epoch_v_g_total = 0\n",
    "    epoch_v_g_style = 0\n",
    "    epoch_v_g_dis = 0\n",
    "    \n",
    "    for step in range(steps):\n",
    "        \n",
    "        idx = step*batch\n",
    "        \n",
    "        if step+1 == steps:\n",
    "            step_train_low = train_low[-batch:]\n",
    "            step_train_high = train_high[-batch:]\n",
    "        else:\n",
    "            step_train_low = train_low[idx:idx+batch]\n",
    "            step_train_high = train_high[idx:idx+batch]\n",
    "            \n",
    "        train_gen_label = np.ones([len(step_train_low), 1], dtype='float')\n",
    "        train_dis_label = np.zeros([len(step_train_low)*2, 1])\n",
    "        train_dis_label[len(step_train_low):] = 1.\n",
    "        \n",
    "        D.trainable=False\n",
    "        \n",
    "        G_Loss = A.train_on_batch(step_train_low, [step_train_high, train_gen_label])\n",
    "        G_output, _= A.predict(step_train_low)\n",
    "        \n",
    "        D.trainable=True\n",
    "        train_dis_input = np.concatenate([step_train_high, G_output], 0)\n",
    "        D_Loss = D.train_on_batch(train_dis_input, train_dis_label)\n",
    "        \n",
    "        epoch_t_g_total += G_Loss[0]\n",
    "        epoch_t_g_style += G_Loss[1]\n",
    "        epoch_t_g_dis += G_Loss[2]\n",
    "        epoch_t_d_dis += D_Loss\n",
    "        \n",
    "        #epoch_progbar.update(step+1, [(\"G_Style\", G_Loss[1]), (\"G_Dis\", G_Loss[2]), (\"D_Dis\", D_Loss)])\n",
    "    \n",
    "    \n",
    "    train_loss[\"Generator_Total\"].append(epoch_t_g_total/steps)\n",
    "    train_loss[\"Generator_Style\"].append(epoch_t_g_style/steps)\n",
    "    train_loss[\"Generator_AD\"].append(epoch_t_g_dis/steps)\n",
    "    train_loss[\"Discriminator_AD\"].append(epoch_t_d_dis/steps)\n",
    "    \n",
    "    V_loss = A.test_on_batch(val_low, [val_high, np.ones([len(val_low), 1], dtype='float')])\n",
    "\n",
    "#     epoch_v_g_total += V_loss[0]\n",
    "#     epoch_v_g_style += V_loss[1]\n",
    "#     epoch_v_g_dis += V_loss[2]    \n",
    "#     val_loss[\"Generator_Total\"].append(epoch_v_g_total/steps)\n",
    "#     val_loss[\"Generator_Style\"].append(epoch_v_g_style/steps)\n",
    "#     val_loss[\"Generator_AD\"].append(epoch_v_g_dis/steps)\n",
    "    val_loss[\"Generator_Total\"].append(V_loss[0])\n",
    "    val_loss[\"Generator_Style\"].append(V_loss[1])\n",
    "    val_loss[\"Generator_AD\"].append(V_loss[2])\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    plt.plot(train_loss['Generator_Style'], '.-')\n",
    "    plt.plot(val_loss['Generator_Style'], '.-')\n",
    "    plt.legend(['Train_Style', 'Validation_Style'], loc=0)\n",
    "    plt.show()\n",
    "\n",
    "    ran_idx = np.random.choice(len(train_low)-1, 1)\n",
    "    test_in = train_low[ran_idx[0]:ran_idx[0]+1]\n",
    "    test, _ = A.predict(test_in)\n",
    "    plot_fn(test, train_high[ran_idx[0]:ran_idx[0]+1])\n",
    "    \n",
    "    \n",
    "    total_progbar.update(epoch+1, [(\"G_Style\", epoch_t_g_style/steps), (\"G_Dis\", epoch_t_g_dis/steps), (\"D_Dis\", epoch_t_d_dis/steps)])\n",
    "    \n",
    "    if (epoch+1)%100 == 0:\n",
    "        A.save_weights('./checkpoint/3to12/dense_MI/%04d_%.2f_%.2f.h5'%(epoch+1, epoch_t_g_style/steps, V_loss[1]))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
